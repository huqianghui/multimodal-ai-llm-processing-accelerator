{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Intelligence Processor Walkthrough\n",
    "This notebook walks through the Document Intelligence processor component that is included in this repository. It is useful for converting the raw API response from Document Intelligence into something more useful, and which can be easily converted into the format required for sending to an LLM endpoint.\n",
    "\n",
    "Some features include:\n",
    "* Automatic extraction of rich content in a PDF, including tables, figures and more.\n",
    "* Output of text content in a PDF, images for each page and figure within the PDF, and pandas dataframes for tables within the PDF.\n",
    "* Automatic correction of image rotation when extracting page and figure images (if not corrected, this can completely destroy LLM extraction accuracy)\n",
    "* Custom definition of the content outputs, allowing for completely dynamic formatting of all content in a file.\n",
    "* Chunking of content into smaller parts (e.g. into chunks of X pages) which can then be processed as part of a Map Reduce pattern.\n",
    "* Automatic conversion of the content to the OpenAI message format, ready for processing with an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient, AnalysisFeature\n",
    "from azure.ai.formrecognizer import AnalyzeResult as FormRecognizerAnalyzeResult\n",
    "from azure.ai.documentintelligence.models import (\n",
    "    AnalyzeResult,\n",
    "    AnalyzeDocumentRequest,\n",
    "    DocumentAnalysisFeature,\n",
    ")\n",
    "\n",
    "# ignore cryptography version warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', module='.*cryptography.*')\n",
    "\n",
    "# Append src module to system path to import from src module\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../function_app\"))\n",
    "\n",
    "from src.components.doc_intelligence import (\n",
    "    DefaultDocumentPageProcessor, DefaultDocumentKeyValuePairProcessor,\n",
    "    DefaultDocumentTableProcessor, DefaultDocumentFigureProcessor,\n",
    "    DefaultDocumentParagraphProcessor, DefaultDocumentLineProcessor,\n",
    "    DefaultDocumentWordProcessor, DefaultSelectionMarkFormatter,\n",
    "    DefaultDocumentSectionProcessor, DocumentIntelligenceProcessor, \n",
    "    PageDocumentListSplitter, convert_processed_di_docs_to_openai_message,\n",
    "    convert_processed_di_docs_to_markdown,\n",
    ")\n",
    "from src.helpers.data_loading import load_pymupdf_pdf, extract_pdf_page_images\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Display all outputs of a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Constants and Define the Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select whether to use the Preview Document Intelligence version (v4.0) or the GA version (v3.3). Each version has some slight differences\n",
    "# - V4.0 is in preview and is only available in a handful of regions. It has some additional features, particularly the ability to process figures within an image.\n",
    "# - V3.3 is Generally Available and available in most Azure regions. It does not have support for extracting/processing figures from within an image.\n",
    "USE_DOC_INTEL_PREVIEW_VERSION = True\n",
    "\n",
    "# Select the model type. \n",
    "# More info here: https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/model-overview\n",
    "DOC_INTEL_MODEL_ID = \"prebuilt-layout\" # E.g. \"prebuilt-read\", \"prebuilt-layout\", or \"prebuilt-document\"\n",
    "\n",
    "# Possible Document Intelligence features\n",
    "# v4.0 (Preview): ['ocrHighResolution', 'languages', 'barcodes', 'formulas', 'keyValuePairs', 'styleFont', 'queryFields']\n",
    "# v3.3 (GA):      ['ocrHighResolution', 'languages', 'barcodes', 'formulas', 'styleFont']\n",
    "\n",
    "DOC_INTEL_FEATURES = ['ocrHighResolution', 'languages', 'styleFont']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables and setup the Document Intelligence client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Document Intelligence Features: ['ocrHighResolution', 'languages', 'styleFont']\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from Function App local settings file\n",
    "with open(\"../function_app/local.settings.json\", \"rb\") as f:\n",
    "    local_settings = json.load(f)\n",
    "    os.environ.update(local_settings[\"Values\"])\n",
    "\n",
    "DOC_INTEL_ENDPOINT = os.getenv(\"DOC_INTEL_ENDPOINT\")\n",
    "DOC_INTEL_API_KEY = os.getenv(\"DOC_INTEL_API_KEY\")\n",
    "\n",
    "# Construct the Document Intelligence clients\n",
    "if USE_DOC_INTEL_PREVIEW_VERSION:\n",
    "    # Doc Intelligence v4.0 (preview) - only available in selected regions\n",
    "    di_client = DocumentIntelligenceClient(\n",
    "        endpoint=DOC_INTEL_ENDPOINT, \n",
    "        credential=AzureKeyCredential(DOC_INTEL_API_KEY),\n",
    "        api_version=\"2024-07-31-preview\",\n",
    "    )\n",
    "    enabled_features = [DocumentAnalysisFeature(feature) for feature in DOC_INTEL_FEATURES]\n",
    "else:\n",
    "    # Doc Intelligence v3.3 (GA) - Available globally\n",
    "    di_client = DocumentAnalysisClient(\n",
    "        endpoint=DOC_INTEL_ENDPOINT, \n",
    "        credential=AzureKeyCredential(DOC_INTEL_API_KEY),\n",
    "        api_version=\"2023-07-31\",\n",
    "    )\n",
    "    enabled_features = [AnalysisFeature(feature) for feature in DOC_INTEL_FEATURES]\n",
    "print(\"Selected Document Intelligence Features:\", [feature.value for feature in enabled_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper objects/functions\n",
    "from typing import Union,Optional\n",
    "import base64\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SamplePdfFileInfo:\n",
    "    name: str\n",
    "    description: str\n",
    "    url_source: Optional[str] = None\n",
    "    file_path: Optional[str] = None\n",
    "\n",
    "def convert_pdf_to_base64(pdf_path: str):\n",
    "    # Read the PDF file in binary mode, encode it to base64, and decode to string\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        base64_encoded_pdf = base64.b64encode(file.read()).decode()\n",
    "    return base64_encoded_pdf\n",
    "\n",
    "def get_analyze_document_result(\n",
    "    sample_pdf_file_info: SamplePdfFileInfo,\n",
    "    di_client: Union[DocumentIntelligenceClient, DocumentAnalysisClient],\n",
    "    model_id: str = \"prebuilt-layout\",\n",
    "    **kwargs\n",
    ") -> Union[AnalyzeResult, FormRecognizerAnalyzeResult]:\n",
    "    \"\"\"\n",
    "    Gets the AnalyzeResult for a sample PDF file using a Document Intelligence\n",
    "    client.\n",
    "    \"\"\"\n",
    "    if isinstance(di_client, DocumentIntelligenceClient):\n",
    "        if sample_pdf_file_info.url_source:\n",
    "            analyze_request = AnalyzeDocumentRequest(url_source=sample_pdf_file_info.url_source)\n",
    "        elif sample_pdf_file_info.file_path:\n",
    "            analyze_request = AnalyzeDocumentRequest(bytes_source=convert_pdf_to_base64(sample_pdf_file_info.file_path))\n",
    "        else:\n",
    "            raise ValueError(\"No valid source provided.\")\n",
    "    \n",
    "        poller = di_client.begin_analyze_document(\n",
    "            model_id=model_id,\n",
    "            analyze_request=analyze_request,\n",
    "            **kwargs\n",
    "        )\n",
    "        new_result = poller.result()\n",
    "    else:\n",
    "        if sample_pdf_file_info.url_source:\n",
    "            poller = di_client.begin_analyze_document_from_url(\n",
    "                model_id=model_id,\n",
    "                document_url=sample_pdf_file_info.url_source,\n",
    "                **kwargs\n",
    "            )\n",
    "        elif sample_pdf_file_info.file_path:\n",
    "            with open(sample_pdf_file_info.file_path, \"rb\") as document:\n",
    "                poller = di_client.begin_analyze_document(\n",
    "                    model_id=model_id,\n",
    "                    document=document,\n",
    "                    **kwargs\n",
    "                )\n",
    "        new_result = poller.result()\n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a list of PDFs for testing\n",
    "We will use a set of PDFs for showcasing how the processor works. These examples include different elements such as tables, inline figures, document structures/heirarchies, and lengths/pages. You can add your own files here for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample PDF files to analyze:\n",
      "- 'oral_cancer_range1-6': An oral cancer description. This document contains 1 pages, consisting of a series of images and tables. This shows how we can extract individual figures,text and tables from the document.\n"
     ]
    }
   ],
   "source": [
    "# # Get raw file links\n",
    "# doc_intelligence_test_files = {\n",
    "#     \"ikea\": SamplePdfFileInfo(\n",
    "#         name=\"IKEA Installation Manual\",\n",
    "#         description=(\n",
    "#             \"An instruction manual for installing an Ikea kitchen. \"\n",
    "#             \"This document contains 12 pages, consisting of a series of diagrams and written instructions. \"\n",
    "#             \"This shows how we can extract individual figures and text from the document.\"\n",
    "#         ),\n",
    "#         url_source=\"https://www.ikea.com/au/en/files/pdf/c7/ef/c7ef4878/kitchen-installation-guide_fy22.pdf\",\n",
    "#     ),\n",
    "#     \"rotated_image_pdf\": SamplePdfFileInfo(\n",
    "#         name=\"Rotated Image PDF\",\n",
    "#         description=(\n",
    "#             \"A single-page PDF containing an embedded image. \"\n",
    "#             \"This document Shows how we can automatically extracted images from a PDF, \"\n",
    "#             \"correcting the rotation of the page and figures images automatically.\"\n",
    "#         ),\n",
    "#         url_source=\"https://github.com/pymupdf/PyMuPDF/blob/main/tests/resources/test_delete_image.pdf?raw=true\",\n",
    "#     ),\n",
    "#     \"editorial_page\": SamplePdfFileInfo(\n",
    "#         name=\"Editorial Page (Mixed content)\",\n",
    "#         description=(\n",
    "#             \"A single-page editorial article with a mix of text and figures on the page. \"\n",
    "#             \"Another example  extracting both text and figures from the page.\"\n",
    "#         ),\n",
    "#         url_source=\"https://github.com/pymupdf/PyMuPDF/blob/main/tests/resources/001003ED.pdf?raw=true\",\n",
    "#     ),\n",
    "#     \"multicolumn_pdf\": SamplePdfFileInfo(\n",
    "#         name=\"Multicolumn PDF with table\",\n",
    "#         description=\"A 3-page test PDF containing multi-column text and a table to be extracted.\",\n",
    "#         url_source=\"https://github.com/py-pdf/sample-files/blob/main/026-latex-multicolumn/multicolumn.pdf?raw=true\",\n",
    "#     ),\n",
    "#     \"rotated_proof_of_delivery_pdf\": SamplePdfFileInfo(\n",
    "#         name=\"Rotated Delivery Receipt\",\n",
    "#         description=\"A 3-page test PDF containing multi-column text and a table to be extracted.\",\n",
    "#         url_source=\"https://github.com/Azure/multimodal-ai-llm-processing-accelerator/blob/main/demo_app/demo_files/Rotated%20Proof%20of%20Delivery%20Receipt.jpg?raw=true\",\n",
    "#     ),\n",
    "# }\n",
    "\n",
    "doc_intelligence_test_files = {\n",
    "    \"oral_cancer_range1-6\": SamplePdfFileInfo(\n",
    "        name=\"adobe convert oral_cancer_text_5th_table&image\",\n",
    "        description=(\n",
    "            \"An oral cancer description. \"\n",
    "            \"This document contains 1 pages, consisting of a series of images and tables. \"\n",
    "            \"This shows how we can extract individual figures,text and tables from the document.\"\n",
    "        ),\n",
    "        file_path=\"/home/azureuser/multimodal-ai-llm-processing-accelerator/multimodel_pdf/oral_cancer_range1-6.pdf\",\n",
    "    )\n",
    "}\n",
    "\n",
    "# Print file links\n",
    "print(\"Sample PDF files to analyze:\")\n",
    "for name, sample_pdf_file_info in doc_intelligence_test_files.items():\n",
    "    print(f\"- '{name}': {sample_pdf_file_info.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Document Intelligence Processor configuration\n",
    "Now we will define the configuration for the processing of the result for Doc Intelligence.\n",
    "\n",
    "Document Intelligence returns many different element objects which correspond to different types of content in the document. Some examples are pages, sections, paragraphs, lines, words, figures, tables and more. Each of these elements have a corresponding processor which is designed to handle that type of element. These processors may simply format the output (e.g. for paragraphs and lines), but others may perform data processing to load and convert the raw API response into something more usable.\n",
    "\n",
    "Here are some key things to look at:\n",
    "\n",
    "##### 1. Processed output dataclass type (Haystack `Document`s)\n",
    "Because the resulting information comes in different types (e.g. text, images, or dataframes), the processors use Haystack's Document dataclass as the standardised data structure for storing the content. These can then be post-processed into other formats such as OpenAI message dictionaries.\n",
    "\n",
    "##### 2. Formatting text content\n",
    "When outputting text, the format of that text can be customized using the `*_text_formats` parameters of each element processor. The format can be defined such that different elements are returned in a specific format (such as italicing text or adding prefixes or suffixes). The format can feature placeholders for different parts of the content, with the list of possible placeholders shown in the constructor docstring for each processor class. \n",
    "* If the processor processes an element that does not contain any information for the placeholder, that format will be skipped. \n",
    "* For example, if the text formats for a figure processor was [\"*Figure Caption:* {caption}\", \"*Figure Footnotes:* {footnotes}\", \"*Figure Content:*\\n{content}\"] but the figure did not have any captions or footnotes, only the final text_format string would be returned in the result. \n",
    "* Final result: \"*Figure Content:\\n<text_extracted_from_inside_the_image>\"\n",
    "\n",
    "##### 3. Outputting and rotating images\n",
    "The Page and Figure processors can automatically output page and figure images into the output, maintaining the correct order of the content. These processors can also automatically adjust the rotation of the image using the `angle` of the page as detected by Document Intelligence. This can ensure that all output images are rotated correctly prior to storing and using those images (this makes a big difference in how accurately LLM's can analyze those images).\n",
    "\n",
    "##### 4. Splitting output `Document`s into chunks\n",
    "When processing large documents, it can be useful to split the original document into separate chunks prior to processing. While the processors are not meant to completely replace other chunking tools (e.g. those in `Langchain` and other libraries), most chunkers are only designed to work with text content and will break if they are given multimodal data. To help avoid this, a `Splitter` can be used to split processed results. The default option is the `PageDocumentListSplitter`, which can split the list of full outputs into chunks based on the page number of the original content. This is useful in cases where you want to take a large Document (e.g. 50+ pages) and process those the document in chunks, then combine the chunk results into a final document-level result. This is a common pattern in document processing (known as the Map Reduce pattern).\n",
    "\n",
    "##### 5. Merging output `Document`s \n",
    "By default, each individual element within a document will result in 1 or more outputs. This means that a single page of paragraphs, lines or words would result in many different output `Document`s. Converting these to LLM messages would then result in hundreds of messages for each document. To prevent this, we can merge adjacent text elements together to reduce the number of text objects prior to converting them into LLM messages. This is accomplished with the \n",
    "* Example result:\n",
    "* Original documents: [text, text, image, text, text, text, table (markdown), text, image]\n",
    "* Merged documents: [text, image, text, image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Intelligence Processor config\n",
    "\n",
    "# Define processors for individual components\n",
    "selection_mark_formatter = DefaultSelectionMarkFormatter(\n",
    "    selected_replacement=\"[X]\", \n",
    "    unselected_replacement=\"[ ]\"\n",
    ")\n",
    "section_processor = DefaultDocumentSectionProcessor(\n",
    "    text_format=None,\n",
    ")\n",
    "page_processor = DefaultDocumentPageProcessor(\n",
    "    page_start_text_formats=[\"\\n*Page {page_number} content:*\"],\n",
    "    page_end_text_formats=None,\n",
    "    page_img_order=\"after\",\n",
    "    page_img_text_intro=\"*Page {page_number} Image:*\",\n",
    "    img_export_dpi=100,\n",
    "    adjust_rotation = True,\n",
    "    rotated_fill_color = (255, 255, 255),\n",
    ")\n",
    "table_processor = DefaultDocumentTableProcessor(\n",
    "    before_table_text_formats=[\"**Table {table_number} Info**\\n\", \"*Table Caption:* {caption}\", \"*Table Footnotes:* {footnotes}\", \"*Table Content:*\"],\n",
    "    after_table_text_formats=None,\n",
    ")\n",
    "figure_processor = DefaultDocumentFigureProcessor(\n",
    "    before_figure_text_formats=[\"**Figure {figure_number} Info**\\n\", \"*Figure Caption:* {caption}\", \"*Figure Footnotes:* {footnotes}\", \"*Figure Content:*\\n{content}\"],\n",
    "    output_figure_img=True,\n",
    "    figure_img_text_format=\"*Figure Image:*\",\n",
    "    after_figure_text_formats=None,\n",
    ")\n",
    "key_value_pair_processor = DefaultDocumentKeyValuePairProcessor(\n",
    "    text_format = \"*Key Value Pair*: {key_content}: {value_content}\",\n",
    ")\n",
    "paragraph_processor = DefaultDocumentParagraphProcessor(\n",
    "    general_text_format = \"{content}\",\n",
    "    page_header_format = None,\n",
    "    page_footer_format = None,\n",
    "    title_format = \"\\n{heading_hashes} **{content}**\",\n",
    "    section_heading_format = \"\\n{heading_hashes} **{content}**\",\n",
    "    footnote_format = \"*Footnote:* {content}\",\n",
    "    formula_format = \"*Formula:* {content}\",\n",
    "    page_number_format = None,\n",
    ")\n",
    "line_processor = DefaultDocumentLineProcessor()\n",
    "word_processor = DefaultDocumentWordProcessor()\n",
    "\n",
    "# Now construct the DocumentIntelligenceProcessor class which uses each of these sub-processors\n",
    "doc_intel_result_processor = DocumentIntelligenceProcessor(\n",
    "    page_processor = page_processor,\n",
    "    section_processor = section_processor,\n",
    "    table_processor = table_processor,\n",
    "    figure_processor = figure_processor,\n",
    "    paragraph_processor = paragraph_processor,\n",
    "    line_processor = line_processor,\n",
    "    word_processor = word_processor,\n",
    "    selection_mark_formatter = selection_mark_formatter\n",
    ")\n",
    "\n",
    "# Now construct the a splitter class which can separate the outputs into different chunks\n",
    "chunk_splitter = PageDocumentListSplitter(pages_per_chunk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Documents\n",
    "We will now process our documents, inspecting the result and converting it into various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== PROCESSING TEST PDF: 'oral_cancer_range1-6' =========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the list of sample PDFs to process\n",
    "# TEST_PDF_NAMES = [\"rotated_proof_of_delivery_pdf\", \"ikea\"] # Select specific test PDFs\n",
    "TEST_PDF_NAMES = list(doc_intelligence_test_files.keys()) # Use all test PDFs\n",
    "\n",
    "# Set the max number of elements to process before stopping (this prevents the notebook from getting too long)\n",
    "BREAK_AFTER_ELEMENT_IDX = 10000\n",
    "\n",
    "for test_pdf_name in TEST_PDF_NAMES:\n",
    "    print(f\"==================== PROCESSING TEST PDF: '{test_pdf_name}' =========================\\n\")\n",
    "    # Load the PDF with PyMuPDF and convert the pages to images. We need to do this to get the images for the pages and figures.\n",
    "    pdf_url = doc_intelligence_test_files[test_pdf_name].url_source\n",
    "    file_path = doc_intelligence_test_files[test_pdf_name].file_path\n",
    "    if pdf_url is not None:\n",
    "        pdf = load_pymupdf_pdf(pdf_path=None, pdf_url=pdf_url)\n",
    "    elif file_path is not None:\n",
    "        pdf = load_pymupdf_pdf(pdf_path=file_path, pdf_url=None)\n",
    "    \n",
    "    doc_page_imgs = extract_pdf_page_images(pdf, img_dpi=100, starting_idx=1)\n",
    "\n",
    "    # Get Doc Intelligence result\n",
    "    di_result = get_analyze_document_result(\n",
    "        sample_pdf_file_info=doc_intelligence_test_files[test_pdf_name],\n",
    "        di_client=di_client,\n",
    "        model_id=DOC_INTEL_MODEL_ID,\n",
    "        features=enabled_features,\n",
    "    )\n",
    "\n",
    "    # Process the API response with the processor\n",
    "    processed_content_docs = doc_intel_result_processor.process_analyze_result(\n",
    "        di_result,\n",
    "        doc_page_imgs=doc_page_imgs, \n",
    "        on_error=\"ignore\", \n",
    "        break_after_element_idx=BREAK_AFTER_ELEMENT_IDX\n",
    "    )\n",
    "\n",
    "    # Split the results into chunks\n",
    "    page_chunked_content_docs = chunk_splitter.split_document_list(processed_content_docs)\n",
    "\n",
    "    # By default, each element outputs a separate data class. Converting these to LLM messages would\n",
    "    # result in hundreds or thousands of messages for each PDF. We can merge adjacent text elements together\n",
    "    # to reduce their quantity prior to converting them into LLM messages.\n",
    "    merged_subchunk_content_docs = doc_intel_result_processor.merge_adjacent_text_content_docs(page_chunked_content_docs, default_text_merge_separator=\"\\n\\n\")\n",
    "\n",
    "    # print(\"Output info:\")\n",
    "    # print(\"\\n- Number of pages in the file:\", len(doc_page_imgs))\n",
    "    # print(\"- Number of content Documents:\", len(processed_content_docs))\n",
    "    # print(\"- Number of content Documents after merging adjacent text Documents:\", sum([len(l) for l in merged_subchunk_content_docs]))\n",
    "    # print(\"- Number of content chunks after splitting the Documents by page:\", len(page_chunked_content_docs))\n",
    "\n",
    "    # Convert content to OpenAI messages\n",
    "    all_content_openai_message = convert_processed_di_docs_to_openai_message(processed_content_docs, role=\"user\")\n",
    "    first_chunk_openai_message = convert_processed_di_docs_to_openai_message(merged_subchunk_content_docs[0], role=\"user\")\n",
    "    \n",
    "    # Optionally print the OpenAI messages\n",
    "    # print(\"\\nAll content OpenAI messages:\")\n",
    "    # all_content_openai_message\n",
    "    # print(\"First chunk OpenAI messages:\")\n",
    "    # first_chunk_openai_message\n",
    "\n",
    "    # Print content in the notebook\n",
    "    print(\"\\nPrinting the content in markdown format\")\n",
    "    for chunk_num, chunk_docs in enumerate(merged_subchunk_content_docs, start=1):\n",
    "        #print(f\"*** Chunk {chunk_num} Content ***\")\n",
    "        md(convert_processed_di_docs_to_markdown(chunk_docs, default_text_merge_separator=\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send the document contents to Azure OpenAI\n",
    "With the document now processed, we can easily convert the output into messages that are ready for processing with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "AOAI_LLM_DEPLOYMENT = os.getenv(\"AOAI_LLM_DEPLOYMENT\")\n",
    "AOAI_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")\n",
    "AOAI_API_KEY = os.getenv(\"AOAI_API_KEY\")\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    azure_endpoint=AOAI_ENDPOINT,\n",
    "    azure_deployment=AOAI_LLM_DEPLOYMENT,\n",
    "    api_key=AOAI_API_KEY,\n",
    "    api_version=\"2024-06-01\",\n",
    "    timeout=30,\n",
    "    max_retries=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content details the subdivision of head and neck nodes into specific anatomical subsites, grouped into seven levels, as seen in Figure 5.1 and Tables 5.1 and 5.2. Additional lymph node groups include suboccipital, retropharyngeal, parapharyngeal, buccinator (facial), preauricular, and periparotid/intraparotid. Histopathologic examination is essential for detecting pathological changes in lymph nodes as imaging cannot identify microscopic tumor foci or distinguish between small reactive and malignant nodes. Table 5.1 outlines the anatomical structures defining the boundaries of the neck levels and sublevels.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Please summarize the content of the following file into 100 words or less.\",\n",
    "    },\n",
    "    first_chunk_openai_message,\n",
    "]\n",
    "response = aoai_client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=AOAI_LLM_DEPLOYMENT\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
